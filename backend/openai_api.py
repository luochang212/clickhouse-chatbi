# -*- coding: utf-8 -*-

"""
Enhanced OpenAI Compatible API for ClickHouse Agent

å®Œå…¨å…¼å®¹ OpenAI API åè®®çš„ ClickHouse Agent æœåŠ¡
æ”¯æŒæµå¼å’Œéæµå¼å“åº”ï¼Œä¿æŒgradioä¸­çš„å›ç­”ç”Ÿæˆæ–¹æ³•
"""

import os
import time
import uuid
import json
from typing import List, Dict, Any, Optional, Union
from datetime import datetime

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from dotenv import load_dotenv

from ch_agent import CHAgent


# åŠ è½½ .env æ–‡ä»¶
load_dotenv()


# LLM é…ç½®
DASHSCOPE_LLM_CFG = {
    'model': 'qwen3-coder-plus',  # qwen-plus / qwen-turbo
    'model_server': os.getenv("DASHSCOPE_BASE_URL"),
    'api_key': os.getenv("DASHSCOPE_API_KEY")
}

DEEPSEEK_LLM_CFG = {
    'model': 'deepseek-reasoner',  # deepseek-chat / deepseek-reasoner
    'model_server': os.getenv("DEEPSEEK_BASE_URL"),
    'api_key': os.getenv("DEEPSEEK_API_KEY")
}


# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    "host": os.getenv("CLICKHOUSE_HOST"),
    "port": os.getenv("CLICKHOUSE_PORT"),
    "user": os.getenv("CLICKHOUSE_USER"),
    "password": os.getenv("CLICKHOUSE_PASSWORD"),
}


# Pydantic æ¨¡å‹å®šä¹‰
class Message(BaseModel):
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: user, assistant, system")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatCompletionRequest(BaseModel):
    model: str = Field(default="clickhouse-agent", description="æ¨¡å‹åç§°")
    messages: List[Message] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼å“åº”")
    temperature: Optional[float] = Field(default=0.7, description="æ¸©åº¦å‚æ•°")
    max_tokens: Optional[int] = Field(default=None, description="æœ€å¤§tokenæ•°")
    top_p: Optional[float] = Field(default=1.0, description="top_på‚æ•°")
    frequency_penalty: Optional[float] = Field(default=0.0, description="é¢‘ç‡æƒ©ç½š")
    presence_penalty: Optional[float] = Field(default=0.0, description="å­˜åœ¨æƒ©ç½š")
    stop: Optional[Union[str, List[str]]] = Field(default=None, description="åœæ­¢è¯")


class ChatCompletionChoice(BaseModel):
    index: int
    message: Message
    finish_reason: str


class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Usage


class ChatCompletionStreamChoice(BaseModel):
    index: int
    delta: Dict[str, Any]
    finish_reason: Optional[str] = None


class ChatCompletionStreamResponse(BaseModel):
    id: str
    object: str = "chat.completion.chunk"
    created: int
    model: str
    choices: List[ChatCompletionStreamChoice]


class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "clickhouse-agent"
    permission: List[Dict] = []
    root: str = "clickhouse-agent"
    parent: Optional[str] = None


class ModelsResponse(BaseModel):
    object: str = "list"
    data: List[ModelInfo]


class ErrorResponse(BaseModel):
    error: Dict[str, Any]


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Enhanced ClickHouse Agent OpenAI Compatible API",
    description="å®Œå…¨å…¼å®¹ OpenAI API åè®®çš„ ClickHouse Agent æœåŠ¡",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“çš„åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# åˆ›å»º Agent å®ä¾‹
def create_react_agent(llm_cfg, db_config):
    """åˆ›å»º React Agent å®ä¾‹"""
    cha = CHAgent(llm_cfg, db_config)
    react_agent = cha.create_react_agent()
    return react_agent


# å…¨å±€ Agent å®ä¾‹
agent = create_react_agent(DASHSCOPE_LLM_CFG, DB_CONFIG)


def generate_response_gradio_style(messages: List[Dict], max_history: int = 6):
    """
    ä½¿ç”¨ä¸gradioç›¸åŒçš„å›ç­”ç”Ÿæˆæ–¹æ³•
    ä¿æŒä¸gradio_ch_agent.pyä¸­generate_responseå‡½æ•°ç›¸åŒçš„é€»è¾‘
    """
    # ä¿ç•™æœ€å max_history æ¡å†å²è®°å½•
    if len(messages) > max_history:
        messages = messages[-max_history:]
    
    # ä½¿ç”¨ Agent ç”Ÿæˆå“åº”
    for chunk in agent.run(messages):
        content = chunk[-1].get("content", "")
        yield content


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "Enhanced ClickHouse Agent OpenAI Compatible API",
        "version": "2.0.0",
        "status": "running",
        "endpoints": {
            "models": "/v1/models",
            "chat_completions": "/v1/chat/completions",
            "health": "/health",
            "docs": "/docs"
        }
    }


@app.get("/v1/models")
async def list_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return ModelsResponse(
        data=[
            ModelInfo(
                id="clickhouse-agent",
                created=int(time.time()),
                owned_by="clickhouse-agent",
                permission=[{
                    "id": "modelperm-clickhouse-agent",
                    "object": "model_permission",
                    "created": int(time.time()),
                    "allow_create_engine": False,
                    "allow_sampling": True,
                    "allow_logprobs": True,
                    "allow_search_indices": False,
                    "allow_view": True,
                    "allow_fine_tuning": False,
                    "organization": "*",
                    "group": None,
                    "is_blocking": False
                }],
                root="clickhouse-agent"
            )
        ]
    )


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    """åˆ›å»ºèŠå¤©å®Œæˆ"""
    try:
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = []
        for msg in request.messages:
            messages.append({
                'role': msg.role,
                'content': msg.content
            })
        
        # ç”Ÿæˆå”¯ä¸€ID
        completion_id = f"chatcmpl-{uuid.uuid4().hex[:29]}"
        created = int(time.time())
        
        if request.stream:
            # æµå¼å“åº”
            return StreamingResponse(
                generate_stream_response(completion_id, created, request.model, messages),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/plain; charset=utf-8"
                }
            )
        else:
            # éæµå¼å“åº”
            response_content = ""
            token_count = 0
            
            # ä½¿ç”¨gradioé£æ ¼çš„å“åº”ç”Ÿæˆæ–¹æ³•
            for content in generate_response_gradio_style(messages):
                response_content = content
                token_count += len(content.split())  # ç®€å•çš„tokenè®¡æ•°
            
            return ChatCompletionResponse(
                id=completion_id,
                created=created,
                model=request.model,
                choices=[
                    ChatCompletionChoice(
                        index=0,
                        message=Message(role="assistant", content=response_content),
                        finish_reason="stop"
                    )
                ],
                usage=Usage(
                    prompt_tokens=sum(len(msg['content'].split()) for msg in messages),
                    completion_tokens=token_count,
                    total_tokens=sum(len(msg['content'].split()) for msg in messages) + token_count
                )
            )
            
    except Exception as e:
        error_response = ErrorResponse(
            error={
                "message": str(e),
                "type": "internal_server_error",
                "param": None,
                "code": "internal_error"
            }
        )
        raise HTTPException(status_code=500, detail=error_response.dict())


async def generate_stream_response(completion_id: str, created: int, model: str, messages: List[Dict]):
    """ç”Ÿæˆæµå¼å“åº”"""
    try:
        # å‘é€å¼€å§‹æ ‡è®°
        start_response = ChatCompletionStreamResponse(
            id=completion_id,
            created=created,
            model=model,
            choices=[
                ChatCompletionStreamChoice(
                    index=0,
                    delta={"role": "assistant", "content": ""},
                    finish_reason=None
                )
            ]
        )
        yield f"data: {start_response.model_dump_json()}\n\n"
        
        # æµå¼ç”Ÿæˆå†…å®¹ - ä½¿ç”¨gradioé£æ ¼çš„å“åº”ç”Ÿæˆæ–¹æ³•
        previous_content = ""
        for current_content in generate_response_gradio_style(messages):
            # è®¡ç®—å¢é‡å†…å®¹
            if current_content.startswith(previous_content):
                delta_content = current_content[len(previous_content):]
            else:
                delta_content = current_content
            
            if delta_content:
                # å‘é€å†…å®¹å—
                content_response = ChatCompletionStreamResponse(
                    id=completion_id,
                    created=created,
                    model=model,
                    choices=[
                        ChatCompletionStreamChoice(
                            index=0,
                            delta={"content": delta_content},
                            finish_reason=None
                        )
                    ]
                )
                yield f"data: {content_response.model_dump_json()}\n\n"
            
            previous_content = current_content
        
        # å‘é€ç»“æŸæ ‡è®°
        end_response = ChatCompletionStreamResponse(
            id=completion_id,
            created=created,
            model=model,
            choices=[
                ChatCompletionStreamChoice(
                    index=0,
                    delta={},
                    finish_reason="stop"
                )
            ]
        )
        yield f"data: {end_response.model_dump_json()}\n\n"
        
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        # å‘é€é”™è¯¯ä¿¡æ¯
        error_data = {
            "error": {
                "message": str(e),
                "type": "internal_server_error",
                "param": None,
                "code": "internal_error"
            }
        }
        yield f'data: {json.dumps(error_data)}\n\n'
        yield "data: [DONE]\n\n"


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    try:
        # ç®€å•çš„å¥åº·æ£€æŸ¥ï¼Œå¯ä»¥æ‰©å±•ä¸ºæ£€æŸ¥æ•°æ®åº“è¿æ¥ç­‰
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "version": "2.0.0",
            "service": "clickhouse-agent",
            "database": "connected" if DB_CONFIG.get("host") else "not_configured"
        }
    except Exception as e:
        raise HTTPException(
            status_code=503,
            detail={
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        )


@app.get("/v1/models/{model_id}")
async def get_model(model_id: str):
    """è·å–ç‰¹å®šæ¨¡å‹ä¿¡æ¯"""
    if model_id == "clickhouse-agent":
        return ModelInfo(
            id="clickhouse-agent",
            created=int(time.time()),
            owned_by="clickhouse-agent",
            permission=[{
                "id": "modelperm-clickhouse-agent",
                "object": "model_permission",
                "created": int(time.time()),
                "allow_create_engine": False,
                "allow_sampling": True,
                "allow_logprobs": True,
                "allow_search_indices": False,
                "allow_view": True,
                "allow_fine_tuning": False,
                "organization": "*",
                "group": None,
                "is_blocking": False
            }],
            root="clickhouse-agent"
        )
    else:
        raise HTTPException(
            status_code=404,
            detail={
                "error": {
                    "message": f"The model '{model_id}' does not exist",
                    "type": "invalid_request_error",
                    "param": None,
                    "code": "model_not_found"
                }
            }
        )


if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ å¯åŠ¨ Enhanced ClickHouse Agent OpenAI Compatible API")
    print("ğŸ“– APIæ–‡æ¡£: http://localhost:8001/docs")
    print("ğŸ” ReDocæ–‡æ¡£: http://localhost:8001/redoc")
    print("ğŸ’š å¥åº·æ£€æŸ¥: http://localhost:8001/health")
    print("ğŸ¤– æ¨¡å‹åˆ—è¡¨: http://localhost:8001/v1/models")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,  # ä½¿ç”¨ä¸åŒçš„ç«¯å£é¿å…å†²çª
        log_level="info",
        reload=False
    )